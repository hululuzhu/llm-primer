# 对技术“弯道超车“ChatGPT的一些思考

全世界都在努力追赶OpenAI的步伐。但是，仅仅复制他们公开的步骤很难取得成功，因为数据反馈存在劣势。因此，必须在某些方面有所"弯道超车"。在此，我大胆猜测一些我个人认为的超车点，供大家娱乐参考。

## 首先是模型预训练过程。
1. GPT3的模型大小和数据量是根据OpenAI自己的scaling law，但在同等算力预算下，也可以修改模型和数据配比。简单来说GPT3用了1750亿参数和2500亿token的数据，如果使用DeepMind Chinchilla的推荐，配比约为1100亿参数和5000亿token。2. 预训练的开销和难度极大，如果急功近利一点，市面上有OPT, bloom和GLM三个千亿参数的超大模型可以照抄。虽然他们都说只能用于科研，但如果真的紧迫，还管这？如果想看起来不是照抄，可以参考OpenAI在搞DOTA2时候的Surgery稍做调整。
3. 模型的架构。如果沿用decoder only，GPT3在原始Transformer基础上对layer norm和residual做了修改，而Google的PaLM对多头注意力中的QKV参数有修改，QK每个头共享参数。位置编码也可以用苏剑林的RoPE。多头注意力最后的线性层也可以修改大小或者激活函数。而如果大家关注过DeepMind的竞赛算法AlphaCode，它其实是一个encoder decoder，虽然它需要模型输出之后的filter和planning辅助，但ChatGPT这样的问答形式做成encoder decoder（或者魔改OPT这样的模型，加入Encoder）也可能是一种思路。
4. 文字占比，GPT3公布了自己文字的来源配比，GPT3.5加入了Github代码集，其他大模型对于训练文字的配比都有各自的配方。如何配比文字集进行训练是门艺术有点品味或者说运气的意思。
5. 模型的训练，一般来说在巨型模型下，优化器和超参数的选择没那么重要，但相比最常用的Adam，还是有可能在质量（RAdam）内存节省（Adafactor）或者超大batch（LAMB）看到一些细微差别。

## 接下来是RL?F的部分。
大家熟悉的是RLHF，H指的是Human，用有限的人类标记训练一个监督奖励模型RM，然后交给RL算法（PPO）来让模型自我提高。但既然要走弯道了，我就扯得远一点。

1. 如果还是走RLHF的排序数据路线，InstructGPT说要6万排序数据可以训练RM，很多人都估计ChatGPT的数据量要多一两个数量级，而OpenAI训练RM的基础模型参数只有60亿（GPT3的1/30），这里可以考虑增加标记数量和增大RM模型的大小来增强RM的能力。因为一般认为RM能力越强，对RL的提高就越有效。
2. 对模型输出文字简单排序是OpenAI的策略，但Anthropic的方法稍有不同，两两比对的时候他们的UI会让选择喜好差别的大小（排序保留了差别，但遗失了差别大小），所以这里需不需要用Anthropic的方式加强信息？
3. H可以换成AI，也就是Anthropic在Constitutional AI中的RLAIF概念，简单来说去掉人类标记实现RM，而用类似宪法大纲的基础让模型自己去评价输出质量实现RM，好处是去掉了人类参与的迭代过程，算力保证的基础上会更简化。而如果Claude效果真的能和ChatGPT匹敌，RLAIF就是算力保证下的更优方案。
4. 更加极端的思路，RL也干掉，只有HF，在人力充足的情况下只用Human Feedback迭代。RL部分非常难训练，所以简单粗暴的话，可以每次SFT训练完一个版本，让海量的人工评价模型输出然后迭代，再训练再迭代，用人海战术实现超车？

# 最后关于ChatGPT加入召回系统，以及应付监管和大众期待
这些都是ChatGPT暂时还没深入的点。
1. 到现在为止ChatGPT模型还没有实时的召回系统整合。如果说Bing搜索是把ChatGPT插入搜索，我指的是要把搜索功能更加整合进语言模型训练过程，这里有Google LaMDA和DeepMind Sparrow参考，也可以直接把搜索结果作为上下文贴在问题前面进行训练。
2. ChatGPT有一个单独API来判断问题有害性，但这个在应付强监管或者符合大众期待方面还是不够的。因为比如我们要屏蔽“王力宏”，ChatGPT能做的就是屏蔽关键字和诱导性问题，但对于“请问谁写了大城小爱这首歌”这个人畜无害的问题，大模型大概率会输出“王力宏”这个敏感词。在ChatGPT之前，其实可以等到结果出来再走一遍审核API，但ChatGPT加入了流输出，就是像人打字一样一段一段显示，这就会对审核模型的实时性有了更高的要求，需要一些创新。
3. 还有一个很有意思的问题就是如何符合大众和监管的期待，比如“俄乌战争的看法”，如果模型一味逃避回答，可能会被认为“没有态度也是一种态度”被批。如果你跟进官方口径，那万一和疫情开放一样突然有天180度转弯怎么办？所以这里可能有两种方案，一种是直接告诉大家这个问题大家有不同看法，然后AI有两个版本，每次都输出两种方案直接骑墙；另一种就是训练模型时候，加入一种超强先验prompt，如果有这个先验prompt（比如“听我说”）模型需要100%服从，这样可以通过简单的prompt技巧在政策180度转弯的时候，无需改变模型参数而实现转舵。
